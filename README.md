# Translatica: English to Spanish Translation

## ğŸ“Œ Overview
Translatica is a Fine-tuned(with PEFT + LoRA) full-stack, LLM-powered translation system. This Machine Translation System capable of translating English text into Spanish using a fine-tuned Transformer-based model. Deployed with a responsive HTML, CSS & Flask, containerized with Docker, and integrated with GitHub CI/CD for continuous delivery.

[![Image](https://github.com/user-attachments/assets/ee8472fe-644e-48e8-adaf-63787a4abd06)](https://github.com/user-attachments/assets/ee8472fe-644e-48e8-adaf-63787a4abd06)

---

## ğŸš€ Live Demo
ğŸ¯ Try the real-time English â†’ Spanish Translator here:  
ğŸ‘‰ [**Translaticaâ€“ Click Here**](https://bilingual-bridge.onrender.com/)

---

## âœ… Real-World Use Case
In today's globalized digital ecosystem, cross-lingual communication is a key challenge. A **customized English-to-Spanish translation model** is highly valuable for:

  * ğŸŒ **Global E-Commerce Platforms** where product descriptions, reviews, and customer support need real-time translation.
  * ğŸ“š **E-learning and EdTech** where English content (like books, tutorials) needs translation into native languages to expand reach.
  * ğŸ¥ **Healthcare Applications** to translate medical information for Spanish-speaking communities.
  * ğŸ“± **Chatbots and Virtual Assistants** that operate in multilingual regions.
  * ğŸ“° **News and Media Localization** where news articles in English are auto-translated for Spanish audiences.

---

## ğŸ“˜ Technical Stack

| Feature                        | Description                                                                 |
| ------------------------------ | --------------------------------------------------------------------------- |
| ğŸ” **Translation**   | Translate fluently English to Spanish                                 |
| ğŸ§  **Fine-Tuned LLM**          | Custom model fine-tuned using **LoRA + PEFT** on bilingual datasets         |
| ğŸ§° **Techniques Used**         | Peft, LoRA (Parameter Efficient Fine-Tuning), SentencePiece Tokenizer        |
| ğŸ–¥ï¸ **Frontend**               | Responsive UI built with **HTML + CSS + JS**, integrated via Flask          |
| ğŸ§ª **Backend API**             | Lightweight Flask server with endpoint for dynamic translation              |
| ğŸ§  **Tokenizer Customization** | Trained custom tokenizers with SentencePiece for improved language fidelity |
| ğŸ’½ **Offline-Ready**           | Entire model is local & GPU-optional â€” designed for CPU-based deployment    |
| ğŸ“¦ **Modular Codebase**        | Fully modular design following clean code & MLOps practices                 |
| ğŸ“Š **Tested Dataset**          | Trained and validated on Bangla-Spanish aligned translation corpus          |
| ğŸ“‰ **Evaluation**     | BLEU Score via `evaluate`       |
| ğŸ”§ **CI/CD Pipeline**   | GitHub Actions (Model Training/Evaluation Workflow)                                |
| ğŸ³ **Containerization** | Docker                                                  |
| ğŸš€ **Hosting Services** | Render   |

---

## âš™ï¸ Tech Stack

* **Model**: [Helsinki-NLP/opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es) (LoRA fine-tuned)
* **Fine-Tuning**: LoRA + PEFT + Hugging Face Trainer
* **Tokenizer**: SentencePiece (custom trained)
* **Frontend**: HTML, CSS, JavaScript
* **Backend**: Flask (Python)
* **Deployment**: CPU-friendly, Flask server
* **Project Structure**: Clean, modular, scalable

---

## ğŸ§  Model Training

* **ğŸ”¡ Dataset:** [`opus_books`](https://huggingface.co/datasets/Helsinki-NLP/opus_books)

  * English-Spanish parallel corpus
  * High-quality literary translation data

* **ğŸ—ï¸ Base Model:** [`Helsinki-NLP/opus-mt-en-es`](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)

  * Pretrained NMT model for English â†’ Spanish

* **ğŸ§ª Tokenizer:**

  * SentencePiece tokenizer (`AutoTokenizer`) from the base model

* **ğŸ§  Fine-Tuning Strategy:**

  * **PEFT (Parameter-Efficient Fine-Tuning)** using **LoRA (Low-Rank Adaptation)**
  * **Frozen base model** â€” only LoRA-injected layers are trainable

* **ğŸ§¬ LoRA Configuration:**

  * `r = 8`, `alpha = 32`, `dropout = 0.1`
  * Target Modules: `["q_proj", "v_proj"]` (attention heads)

* **âš™ï¸ Trainer Configuration:**

  * Epochs: `3`
  * Batch Size: `16`
  * Learning Rate: `2e-5`
  * Weight Decay: `0.01`
  * Evaluation: Per Epoch
  * Early Stopping: Enabled
  * Mixed Precision: Enabled (`fp16`)

* **ğŸ“Š Evaluation Metric:**

  * BLEU Score using Hugging Face `evaluate` library

* **ğŸ“‰ Training Performance:**

| Epoch | Training Loss | Validation Loss | BLEU Score |
| ----- | ------------- | --------------- | ---------- |
| 1     | 1.3853        | 1.3317          | 0.1164     |
| 2     | 1.3521        | 1.2777          | 0.1136     |
| 3     | 1.3885        | 1.2645          | 0.1128     |

* **ğŸ’¾ Saving Checkpoints:**

  * Tokenizer: `tokenizer.save_pretrained("saved_tokenizer")`
  * LoRA Adapter: `peft_model.save_pretrained("saved_peft_model")`

* **ğŸ“¦ Trainable Parameters:**

  * Only \~0.38% of parameters were updated
  * Verified using: `peft_model.print_trainable_parameters()`

---

## ğŸ“Š Model Performance

| Epoch | Training Loss | Validation Loss | BLEU Score |
|-------|---------------|-----------------|------------|
| 1     | 1.3853        | 1.3317          | 0.1164     |
| 2     | 1.3521        | 1.2777          | 0.1136     |
| 3     | 1.3885        | 1.2645          | 0.1128     |

ğŸ“Œ **Observations**:
- Training and validation loss gradually decreased.
- BLEU score stabilized over epochs, indicating early convergence.
- Indicates the model is learning but BLEU can be improved further with more epochs or tuning.

---

## ğŸ§± Project File Structure
```
Bilingual-Bridge/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ main.yml
â”‚
â”œâ”€â”€ fine-tuned-model/
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ fine-tuned-tokenizer/
â”‚   â”œâ”€â”€ source.spm
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ target.spm
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ vocab.json
â”‚
â”œâ”€â”€ logs/
â”‚
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ Experiment.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ model.py
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ script.js
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ tests/
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ app.png
â”œâ”€â”€ demo.mp4
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ train.py
```

---

## ğŸ“± System Architecture 
```mermaid
sequenceDiagram
    participant User
    participant WebUI as Web UI (Flask + HTML/CSS)
    participant TranslationServer as Translation App (Python)
    participant HFModel as Hugging Face Model

    User->>WebUI: Enter English text
    WebUI->>TranslationServer: translate(input_text)
    TranslationServer->>HFModel: Tokenize input
    HFModel-->>TranslationServer: Return tokenized input
    TranslationServer->>HFModel: Generate translation tokens
    HFModel-->>TranslationServer: Return translated tokens
    TranslationServer-->>WebUI: Return Spanish text
    WebUI->>User: Display translated output
```

---

## ğŸ³ Docker Support
```Dockerfile
FROM python:3.10
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
CMD ["python", "app.py"]
```

Build & run:
```bash
docker build -t translatica .
docker run -p 8501:8501 translatica
```

---

## ğŸ”® Future Enhancements
- Add multilingual support (more language pairs)
- Quantize and optimize for edge devices

---

## ğŸ“‚ How to Run (Locally)
```bash
git clone https://github.com/Md-Emon-Hasan/Bilingual-Bridge
cd bilingual-bridge
pip install -r requirements.txt
streamlit run app.py
```
---

## âœï¸ Prepared by  

**Md Emon Hasan**  
ğŸ“§ **Email:** iconicemon01@gmail.com  
ğŸ’¬ **WhatsApp:** [+8801834363533](https://wa.me/8801834363533)  
ğŸ”— **GitHub:** [Md-Emon-Hasan](https://github.com/Md-Emon-Hasan)  
ğŸ”— **LinkedIn:** [Md Emon Hasan](https://www.linkedin.com/in/md-emon-hasan-695483237/)  
ğŸ”— **Facebook:** [Md Emon Hasan](https://www.facebook.com/mdemon.hasan2001/)

---